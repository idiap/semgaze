{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "from semgaze.modeling.semgaze import SemGaze\n",
    "from semgaze.utils.common import dark_coordinate_decoding, square_bbox\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "DET_TRH = 0.4\n",
    "IMG_MEAN = [0.44232, 0.40506, 0.36457]\n",
    "IMG_STD = [0.28674, 0.27776, 0.27995]\n",
    "\n",
    "COLOR_NAMES = [\"mediumvioletred\", \"green\", \"dodgerblue\", \"crimson\", \"goldenrod\", \n",
    "               \"DarkSlateGray\", \"saddlebrown\", \"purple\", \"teal\"]\n",
    "COLORS = [(199, 21, 133), (0, 128, 0), (30, 144, 255), (220, 20, 60), (218, 165, 32), \n",
    "          (47, 79, 79), (139, 69, 19), (128, 0, 128), (0, 128, 128)]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(module):\n",
    "    return sum([param.numel() for param in module.parameters()])\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, img_w, img_h, k=0.1):\n",
    "    w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "    bbox[0] = max(0, bbox[0] - k * w)\n",
    "    bbox[1] = max(0, bbox[1] - k * h)\n",
    "    bbox[2] = min(img_w, bbox[2] + k * w)\n",
    "    bbox[3] = min(img_h, bbox[3] + k * h)\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def load_head_detection_model(device):\n",
    "    # Load and return the pre-trained head detection model\n",
    "    ckpt_path = \"weights/yolo11m_merged.torchscript\"\n",
    "    model = YOLO(ckpt_path, task=\"detect\")\n",
    "    return model\n",
    "\n",
    "def load_text_model(device):\n",
    "    # Load CLIP Text Encoder\n",
    "    text_model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    text_model.eval()\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    return text_model, text_tokenizer\n",
    "\n",
    "def load_semgaze_model(ckpt_path, device):\n",
    "    # Build model\n",
    "    semgaze = SemGaze(\n",
    "        image_size = 256,\n",
    "        patch_size = 16,\n",
    "        token_dim = 768,\n",
    "        gaze_vec_dim = 2,\n",
    "        encoder_num_heads = 12,\n",
    "        encoder_depth = 12,\n",
    "        encoder_num_global_tokens = 1,\n",
    "        decoder_depth = 2,\n",
    "        decoder_num_heads = 8,\n",
    "        decoder_label_emb_dim = 512,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    logit_scale = checkpoint[\"state_dict\"][\"logit_scale\"]\n",
    "    state_dict = {name[6:]: value for name, value in checkpoint[\"state_dict\"].items() if name != \"logit_scale\"}\n",
    "    semgaze.load_state_dict(state_dict, strict=True)\n",
    "    semgaze.to(device)\n",
    "    semgaze.eval()\n",
    "    return semgaze, logit_scale\n",
    "\n",
    "\n",
    "def draw_gaze(\n",
    "    image,\n",
    "    head_bboxes,\n",
    "    gaze_points,\n",
    "    gaze_vecs,\n",
    "    inouts,\n",
    "    pids,\n",
    "    gaze_heatmaps,\n",
    "    heatmap_pid = None,\n",
    "    frame_nb = None,\n",
    "    colors = COLORS,\n",
    "    alpha: float = 0.5,\n",
    "    io_thr: float = 0.4, \n",
    "    gaze_pt_size: int = 10,\n",
    "    gaze_vec_factor: float = 0.8,\n",
    "    head_center_size: int = 10,\n",
    "    thickness: int = 4,\n",
    "    fs: float = 0.6,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws gaze results on the given image.\n",
    " \n",
    "    Args:\n",
    "        image (np.ndarray): The input image on which to draw.\n",
    "        head_bboxes (array-like): Bounding boxes for heads.\n",
    "        gaze_points (array-like): Points representing gaze locations.\n",
    "        gaze_vecs (array-like): Vectors representing gaze directions.\n",
    "        inouts (array-like): In/out scores for each head.\n",
    "        pids (array-like): Person IDs for each head.\n",
    "        gaze_heatmaps (array-like): Heatmaps for gaze.\n",
    "        heatmap_pid (int, optional): Person ID for which to draw the heatmap. Defaults to None.\n",
    "        frame_nb (int, optional): Frame number to display on the image. Defaults to None.\n",
    "        colors (array-like, optional): Colors to use for drawing. Defaults to COLORS.\n",
    "        alpha (float, optional): Alpha blending value for heatmap overlay. Defaults to 0.5.\n",
    "        io_thr (float, optional): Threshold for in/out scores to draw gaze points. Defaults to 0.5.\n",
    "        gaze_pt_size (int, optional): Size of the gaze points. Defaults to 10.\n",
    "        gaze_vec_factor (float, optional): Scaling factor for gaze vectors. Defaults to 0.8.\n",
    "        head_center_size (int, optional): Size of the head center points. Defaults to 10.\n",
    "        thickness (int, optional): Thickness of the drawing lines. Defaults to 4.\n",
    "        fs (float, optional): Font scale for text. Defaults to 0.6.\n",
    "    Returns:\n",
    "        np.ndarray: The image with gaze results drawn on it.\n",
    "    \"\"\"\n",
    "    # Create canvas on which to draw predictions\n",
    "    img_h, img_w, img_c = image.shape\n",
    "    canvas = image.copy()\n",
    "    \n",
    "    # Scale of the drawing according to image resolution\n",
    "    scale = max(img_h, img_w) / 1920\n",
    "    fs *= scale\n",
    "    thickness = int(scale * thickness)\n",
    "    gaze_pt_size = int(scale * gaze_pt_size)\n",
    "    head_center_size = int(scale * head_center_size)\n",
    "    \n",
    "    # Draw heatmap\n",
    "    if heatmap_pid is not None:\n",
    "        if len(gaze_heatmaps) == 0:\n",
    "            raise ValueError(\"gaze_heatmaps must be provided if heatmap_pid is provided.\")\n",
    "        mask = (pids == heatmap_pid)\n",
    "        if mask.sum() == 1: # only if detection found\n",
    "            gaze_heatmap = gaze_heatmaps[mask]\n",
    "            heatmap = TF.resize(gaze_heatmap, (img_h, img_w), antialias=True).squeeze().numpy()\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "            heatmap = cm.inferno(heatmap) * 255 \n",
    "            canvas = ((1 - alpha) * image + alpha * heatmap[..., :3]).astype(np.uint8)\n",
    "\n",
    "            # Write pid being used for the heatmap\n",
    "            hm_pid_text = f\"Heatmap PID: {heatmap_pid}\"\n",
    "            (w_text, h_text), _ = cv2.getTextSize(hm_pid_text, cv2.FONT_HERSHEY_SIMPLEX, fs, 1)\n",
    "            ul = (img_w - w_text - 20, img_h - h_text - 15)\n",
    "            br = (img_w, img_h)\n",
    "            cv2.rectangle(canvas, ul, br, (0, 0, 0), -1)\n",
    "            hm_pid_text_loc = (img_w - w_text - 10, img_h - 10)\n",
    "            cv2.putText(canvas, hm_pid_text, hm_pid_text_loc, cv2.FONT_HERSHEY_SIMPLEX, fs, (255, 255, 255), 1, cv2.LINE_AA)   \n",
    "\n",
    "    # Draw head bboxes  \n",
    "    if len(head_bboxes) > 0:\n",
    "        if len(pids) == 0:\n",
    "            raise ValueError(\"pids must be provided if head_bboxes is provided.\")\n",
    "        \n",
    "        # Convert to numpy\n",
    "        head_bboxes = head_bboxes.numpy() if isinstance(head_bboxes, torch.Tensor) else np.array(head_bboxes)\n",
    "        inouts = inouts.numpy() if isinstance(inouts, torch.Tensor) else np.array(inouts)\n",
    "        if head_bboxes.max() <= 1.0:\n",
    "            head_bboxes = head_bboxes * np.array([img_w, img_h, img_w, img_h])\n",
    "        head_bboxes = head_bboxes.astype(int)\n",
    "        \n",
    "        # Compute head center\n",
    "        head_centers = np.hstack([(head_bboxes[:,[0]] + head_bboxes[:,[2]]) / 2, (head_bboxes[:,[1]] + head_bboxes[:,[3]]) / 2])\n",
    "        head_centers = head_centers.astype(int)\n",
    "        \n",
    "        gaze_available = (len(gaze_points) > 0)\n",
    "        if gaze_available and (len(inouts) == 0):\n",
    "            raise ValueError(\"inouts must be provided if gaze_pts is provided.\")\n",
    "            \n",
    "        if gaze_available:\n",
    "            gaze_points = gaze_points.numpy() if isinstance(gaze_points, torch.Tensor) else np.array(gaze_points)\n",
    "            if (gaze_points.max() <= 1.):\n",
    "                gaze_points = gaze_points * np.array([img_w, img_h])\n",
    "            gaze_points = gaze_points.astype(int)\n",
    "            \n",
    "        if gaze_vecs is not None:\n",
    "            gaze_vecs = gaze_vecs.numpy() if isinstance(gaze_vecs, torch.Tensor) else np.array(gaze_vecs)\n",
    "        \n",
    "        for i, head_bbox in enumerate(head_bboxes):\n",
    "            \n",
    "            if (heatmap_pid is not None) and (heatmap_pid != i):\n",
    "                continue\n",
    "            \n",
    "            xmin, ymin, xmax, ymax = head_bbox\n",
    "            head_radius = max(xmax-xmin, ymax-ymin) // 2\n",
    "            pid = pids[i]\n",
    "            color = colors[pid % len(colors)]\n",
    "                            \n",
    "            # Compute Head Center\n",
    "            head_center = head_centers[i]\n",
    "        \n",
    "            head_bbox_ul = (xmin, ymin)\n",
    "            head_bbox_br = (xmax, ymax)\n",
    "            head_center_ul = head_center - (head_center_size // 2)\n",
    "            head_center_br = head_center + (head_center_size // 2)\n",
    "            cv2.rectangle(canvas, head_center_ul, head_center_br, color, -1) # head center point\n",
    "            cv2.circle(canvas, head_center, head_radius, color, thickness) # head circle\n",
    "            \n",
    "            # Draw header\n",
    "            io = inouts[i] if inouts is not None else \"-\"\n",
    "            header_text = f\"P{pid}: {io:.2f}\"\n",
    "            (w_text, h_text), _ = cv2.getTextSize(header_text, cv2.FONT_HERSHEY_SIMPLEX, fs, 1)\n",
    "            \n",
    "            header_ul =  (int(head_center[0] - w_text / 2), int(ymin - thickness / 2))\n",
    "            header_br = (int(head_center[0] + w_text / 2), int(ymin + h_text + 5))\n",
    "            cv2.rectangle(canvas, header_ul, header_br, color, -1) # header bbox\n",
    "            cv2.putText(canvas, header_text, (header_ul[0], int(ymin + h_text)), cv2.FONT_HERSHEY_SIMPLEX, fs, (255, 255, 255), 1, cv2.LINE_AA) # header text\n",
    "            \n",
    "            if gaze_available and (io > io_thr):\n",
    "                gp = gaze_points[i]\n",
    "                vec = (gp - head_center)\n",
    "                vec = vec / (np.linalg.norm(vec) + 0.000001)\n",
    "                intersection = head_center + (vec * head_radius).astype(int)\n",
    "                #cv2.line(canvas, head_center, gp, color, int(0.5 * thickness)) # UNCOMMENT\n",
    "                cv2.line(canvas, intersection, gp, color, thickness)\n",
    "                \n",
    "                cv2.circle(canvas, gp, gaze_pt_size, color, -1)\n",
    "                \n",
    "            if gaze_vecs is not None:\n",
    "                gv = gaze_vecs[i]\n",
    "                cv2.arrowedLine(canvas, head_center, (head_center + gaze_vec_factor * head_radius * gv).astype(int), color, thickness)\n",
    "                \n",
    "                \n",
    "    # Write frame number\n",
    "    if frame_nb is not None:\n",
    "        frame_nb = str(frame_nb)\n",
    "        (w_text, h_text), _ = cv2.getTextSize(frame_nb, cv2.FONT_HERSHEY_SIMPLEX, fs, 1)\n",
    "        nb_ul = (int((img_w - w_text) / 2), (img_h - h_text - 15))\n",
    "        nb_br = (int((img_w + w_text) / 2), img_h)\n",
    "        cv2.rectangle(canvas, nb_ul, nb_br, (0, 0, 0), -1)\n",
    "        nb_text_loc = (int((img_w - w_text) / 2), (img_h - 10))\n",
    "        cv2.putText(canvas, frame_nb, nb_text_loc, cv2.FONT_HERSHEY_SIMPLEX, fs, (255, 255, 255), 1, cv2.LINE_AA) \n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "\n",
    "def predict_gaze(img_path, semgaze, head_detector):\n",
    "    # 1. Read image\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    img_h, img_w, img_c = image_np.shape\n",
    "\n",
    "    # 2. detect & process head bboxes\n",
    "    detections = head_detector(image, verbose=False)[0]\n",
    "    head_bboxes = []\n",
    "    for bbox, conf in zip(detections.boxes.xyxy, detections.boxes.conf):\n",
    "        if conf > DET_TRH:\n",
    "            #bbox = expand_bbox(bbox.cpu(), img_w, img_h, k=0.1)\n",
    "            head_bboxes.append(bbox)     \n",
    "    head_bboxes = torch.stack(head_bboxes).cpu()\n",
    "    t_head_bboxes = square_bbox(head_bboxes, img_w, img_h)\n",
    "\n",
    "    num_heads = len(head_bboxes)\n",
    "    print(f\"Detected {num_heads} heads.\")\n",
    "\n",
    "    # 3. Extract and transform heads\n",
    "    heads = []\n",
    "    for bbox in t_head_bboxes:\n",
    "        head = TF.resize(TF.to_tensor(image.crop(bbox.numpy())), (224, 224))\n",
    "        heads.append(head)\n",
    "    heads = torch.stack(heads)\n",
    "    heads = TF.normalize(heads, mean=IMG_MEAN, std=IMG_STD)\n",
    "\n",
    "    # 4. Transform Image\n",
    "    image = TF.to_tensor(image)\n",
    "    image = TF.resize(image, (256, 256))\n",
    "    image = TF.normalize(image, mean=IMG_MEAN, std=IMG_STD)\n",
    "\n",
    "    # 5. Normalize head bboxes\n",
    "    scale = torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    t_head_bboxes /= scale\n",
    "\n",
    "    # 6. build input sample\n",
    "    sample = {}\n",
    "    sample[\"image\"] = image.unsqueeze(0).expand(num_heads, -1, -1, -1).to(device) # (num_heads, 1, 3, 224, 224)\n",
    "    sample[\"heads\"] = heads.unsqueeze(1).to(device) # (num_heads, 1, 3, 224, 224)\n",
    "    sample[\"head_bboxes\"] = t_head_bboxes.unsqueeze(1).to(device) # (num_heads, 1, 4)\n",
    "\n",
    "    # 7. predict gaze\n",
    "    with torch.no_grad():\n",
    "        gaze_heatmaps, gaze_vecs, gaze_label_embs = semgaze(sample)\n",
    "        gaze_heatmaps = gaze_heatmaps.squeeze(1).cpu()\n",
    "        gaze_vecs = gaze_vecs.squeeze(1).cpu()    \n",
    "        gaze_points = dark_coordinate_decoding(gaze_heatmaps, kernel_size=9, normalize=True)\n",
    "        gaze_label_embs = gaze_label_embs.squeeze(1).cpu()\n",
    "  \n",
    "    return image_np, head_bboxes, gaze_points, gaze_vecs, gaze_heatmaps, gaze_label_embs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Head Detector\n",
    "head_detector = load_head_detection_model(device)\n",
    "head_detector_num_params = get_num_params(head_detector)\n",
    "head_detector_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SemGaze model\n",
    "ckpt_path = \"checkpoints/gazefollow.ckpt\"\n",
    "\n",
    "semgaze, logit_scale = load_semgaze_model(ckpt_path, device)\n",
    "semgaze_num_params = get_num_params(semgaze)\n",
    "semgaze_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text model\n",
    "text_model, text_tokenizer = load_text_model(device)\n",
    "\n",
    "# Compute vocabulary class embeddings (we use COCO classes here, but you can use any other set of classes)\n",
    "VOCAB = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n",
    "    'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "    'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "    'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n",
    "    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "    'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n",
    "    'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "    'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "    'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "vocab_embs = []\n",
    "with torch.no_grad():\n",
    "    for class_name in VOCAB:\n",
    "        inputs = text_tokenizer(class_name, return_tensors=\"pt\")\n",
    "        outputs = text_model(**inputs)\n",
    "        class_emb = outputs.text_embeds.squeeze(0)\n",
    "        vocab_embs.append(class_emb)\n",
    "\n",
    "# Normalize Class Embeddings\n",
    "vocab_embs = F.normalize(torch.stack(vocab_embs), p=2, dim=-1)\n",
    "vocab_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and execute this cell once so matplotlib will display the plots inline\n",
    "#%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict gaze for a given image\n",
    "img_path = \"samples/image.jpg\" # change to the path of the image you want to test e.g. \"data/image2.jpg\" or \"data/image3.jpg\"\n",
    "output = predict_gaze(img_path, semgaze, head_detector)\n",
    "image_np, head_bboxes, gaze_points, gaze_vecs, gaze_heatmaps, gaze_label_embs = output\n",
    "\n",
    "# Convert gaze label embeddings to class probabilities\n",
    "gaze_label_logits = gaze_label_embs @ vocab_embs.T * logit_scale.data.exp()\n",
    "gaze_label_probs = gaze_label_logits.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_to_labels(probs, vocab, top_k=3):\n",
    "    labels = {}\n",
    "    for pid, prob in enumerate(probs):\n",
    "        values, sort_indices = prob.sort(descending=True)\n",
    "        top_k_labels = [(vocab[idx], round(value.item(), 3)) for value, idx in zip(values[:top_k], sort_indices[:top_k])]\n",
    "        labels[pid] = top_k_labels\n",
    "    return labels\n",
    "\n",
    "probs_to_labels(gaze_label_probs, VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "show_gaze_vec = True\n",
    "alpha = 0.7\n",
    "fs = 1.\n",
    "thickness = 10\n",
    "gaze_pt_size = 20\n",
    "head_center_size = 18\n",
    "gaze_vec_factor = 0.6\n",
    "\n",
    "img_h, img_w = image_np.shape[:2]\n",
    "num_people = len(head_bboxes)\n",
    "pids = np.arange(num_people)\n",
    "\n",
    "num_axes = 2 + num_people\n",
    "ncols = 2\n",
    "nrows = np.ceil(num_axes / ncols).astype(int)\n",
    "fig_w = 20\n",
    "ax_w = fig_w // ncols\n",
    "ax_h = int(round(ax_w * img_h / img_w))\n",
    "\n",
    "fig, axes = plt.subplots(figsize = (fig_w, ax_h * nrows), nrows = nrows, ncols = ncols, tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "[ax.axis(\"off\") for ax in axes]\n",
    "\n",
    "# Show input image\n",
    "axes[0].imshow(image_np)\n",
    "\n",
    "# Iterate over people and show the heatmap of each. The first iteration (ie. None) shows all predictions without heatmaps\n",
    "i = 1\n",
    "for heatmap_pid in [None] + np.arange(num_people).tolist():\n",
    "    frame = draw_gaze(image_np, \n",
    "                      head_bboxes = head_bboxes, \n",
    "                      gaze_points = gaze_points, \n",
    "                      gaze_vecs = gaze_vecs if show_gaze_vec else None, \n",
    "                      inouts = np.ones(num_people), \n",
    "                      pids = pids, \n",
    "                      gaze_heatmaps = gaze_heatmaps, \n",
    "                      heatmap_pid = heatmap_pid, \n",
    "                      frame_nb = None, \n",
    "                      colors = COLORS,\n",
    "                      alpha = alpha, \n",
    "                      gaze_pt_size = gaze_pt_size,\n",
    "                      gaze_vec_factor = gaze_vec_factor,\n",
    "                      head_center_size = head_center_size,\n",
    "                      thickness = thickness,\n",
    "                      fs = fs,\n",
    "                     ) \n",
    "\n",
    "    axes[i].imshow(frame)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rinnegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
